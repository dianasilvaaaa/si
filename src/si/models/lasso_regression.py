import numpy as np
from si.base.model import Model
from si.data.dataset import Dataset
from si.metrics.mse import mse


class LassoRegression(Model):
    """
    Regress√£o Lasso com regulariza√ß√£o L1 para ajustar coeficientes.
    """

    def __init__(self, l1_penalty: float = 1, max_iter: int = 1000, patience: int = 5, scale: bool = True, **kwargs):
        """
        Inicializa o modelo Lasso Regression.

        Par√¢metros:
        ----------
        l1_penalty : float
            Par√¢metro de regulariza√ß√£o L1.A penaliza√ß√£o L1 (par√¢metro de regulariza√ß√£o) para controle da magnitude dos coeficientes.
        max_iter : int
            N√∫mero m√°ximo de itera√ß√µes que o modelo deve realizar durante o ajuste dos par√¢metros.
        patience : int
            N√∫mero de itera√ß√µes sem melhoria no erro para ativar o early stopping (interrup√ß√£o antecipada).
        scale : bool
           e verdadeiro, os dados s√£o normalizados (m√©dia 0 e desvio padr√£o 1).

          Atributos:
        ----------
        theta : np.ndarray
            Coeficientes do modelo (par√¢metros ajustados durante o treinamento).
        theta_zero : float
            Intercepto (coeficiente de vi√©s).
        mean : np.ndarray
            M√©dia de cada coluna dos dados (usada para normaliza√ß√£o).
        std : np.ndarray
            Desvio padr√£o de cada coluna dos dados (usado para normaliza√ß√£o).
        cost_history : dict
            Hist√≥rico do custo (erro) durante as itera√ß√µes.
        """

        super().__init__(**kwargs) # Inicializa qualquer comportamento herdado de Model
        self.l1_penalty = l1_penalty # Define o par√¢metro de penaliza√ß√£o L1
        self.max_iter = max_iter # Define o n√∫mero m√°ximo de itera√ß√µes
        self.patience = patience # Define o limite de itera√ß√µes sem melhoria
        self.scale = scale # Indica se os dados devem ser normalizados

        # Inicializa os atributos do modelo
        self.theta = None # Coeficientes iniciais (n√£o definidos at√© o treinamento)
        self.theta_zero = None # Intercept inicial
        self.mean = None # M√©dia para normaliza√ß√£o (calculada no treinamento)
        self.std = None # Desvio padr√£o para normaliza√ß√£o (calculado no treinamento)
        self.cost_history = {} # Dicion√°rio para armazenar o hist√≥rico do custo

    def _fit(self, dataset: Dataset) -> 'LassoRegression':
        """
        Ajusta o modelo aos dados fornecidos.

        Par√¢metros:
        ----------
        dataset : Dataset
            Conjunto de dados de entrada.

        Retorna:
        -------
        self : LassoRegression
            O modelo ajustado.
        """
        # Verifica se os dados precisam ser normalizados
        # A normaliza√ß√£o √© feita subtraindo a m√©dia e dividindo pelo desvio padr√£o.
        """
        Se o par√¢metro scale=True, os dados de entrada (dataset.X) s√£o normalizados (cada vari√°vel tem m√©dia 0 e desvio padr√£o 1). 
        A vari√°vel mean armazena a m√©dia de cada coluna de X, e std armazena o desvio padr√£o de cada coluna.
        Se a normaliza√ß√£o n√£o for necess√°ria (scale=False), a vari√°vel X simplesmente recebe os dados de entrada como est√£o

        """

#1 NORMALIZA√á√ÉO DOS DADOS: 
#Objetivo: Garantir que todas as vari√°veis ùëãùëó tenham m√©dia zero e desvio padr√£o unit√°rio, evitando que vari√°veis com escalas maiores dominem o ajuste do modelo.
# Normaliza os dados (X) se necess√°rio, garantindo que tenham m√©dia zero e desvio padr√£o unit√°rio.
        if self.scale:
            self.mean, self.std = dataset.X.mean(axis=0), dataset.X.std(axis=0) # Calcula m√©dia e desvio padr√£o
            X = (dataset.X - self.mean) / (self.std + 1e-8)   # Normaliza os dados 
        else:
            X = dataset.X  
#calcula-se a media e o desvio padr√£o. Xnormalizado = (X-media)/desvio padrao



#2 INICIALIZA√á√ÉO DOS PAR√ÇMETROS:
#Objetivo: Come√ßar com valores neutros para os coeficientes (ùúÉùëó) e o intercepto (ùúÉ0).
#Inicializa os coeficientes (theta) e o intercepto (theta_zero) com zeros.
        m, n = X.shape # Obt√©m o n√∫mero de amostras (m) e caracter√≠sticas (n)
        # Inicializa os par√¢metros
        # m e n representam, respectivamente, o n√∫mero de amostras (linhas) 
        # e n n√∫mero de caracter√≠sticas (colunas) de X.
        self.theta = np.zeros(n) # √© um vetor de coeficientes do modelo, e √© inicializado com zeros. Ele ter√° o mesmo n√∫mero de elementos que o n√∫mero de caracter√≠sticas em X.
        self.theta_zero = 0 #√© o intercepto do modelo (o termo constante), que tamb√©m √© inicializado com 0.
        
        
        """
O modelo √© treinado utilizando gradiente descendente, um algoritmo de otimiza√ß√£o usado para minimizar a fun√ß√£o de custo. 
O gradiente √© calculado em cada itera√ß√£o para ajustar os coeficientes de modo que o erro seja minimizado.

A cada itera√ß√£o, o modelo faz uma previs√£o (y_pred) com os coeficientes theta e o intercept theta_zero.
A previs√£o y_pred √© calculada como o produto escalar entre os dados X e os coeficientes theta, somando o intercept theta_zero.
        """

#3 COORDENADA DESCENDENTE
#Loop de treinamento que ajusta os coeficientes
#Calcula as previs√µes (y_pred) com base nos coeficientes atuais.         

        early_stopping_counter = 0 # Contador para o mecanismo de early stopping
        for iteration in range(self.max_iter): # Loop principal para ajustar os par√¢metros

    #3.1    #calculo das previs√µes: ypred = ùúÉ0 + media(ùúÉùëó x Xj)
            y_pred = np.dot(X, self.theta) + self.theta_zero # Calcula as previs√µes baseadas no modelo atual

    #3.2    # Atualiza o theta_zero com base na m√©dia dos res√≠duos: media (y-ypred)
            self.theta_zero = np.mean(dataset.y - y_pred) #O intercepto √© ajustado calculando a m√©dia da diferen√ßa entre as previs√µes e os valores reais (dataset.y).

    #3.3    #Atualiza√ß√£o de cada coeficiente (ùúÉùëó)
            # Atualiza os coeficientes usando o gradiente descendente com regualariza√ß√£o L1. 
            for j in range(n): # Para cada coeficiente:
                X_feature = X[:, j] # Seleciona a caracter√≠stica j
    
    #3.3.1  #C√°lculo do res√≠duo ajustado para ùúÉùëó
                residual = dataset.y - (y_pred - X_feature * self.theta[j]) # Calcula o res√≠duo excluindo a contribui√ß√£o de j
                gradient = np.dot(X_feature, residual) / m # Gradiente para o coeficiente j

    #3.3.2  #Atualiza√ß√£o usando o soft thresholding
                 # Atualiza o coeficiente aplicando a regulariza√ß√£o ùêø1(penaliza√ß√£o Lasso):
                self.theta[j] = self._apply_soft_threshold(gradient, self.l1_penalty) / (np.dot(X_feature, X_feature) / m)
            """
Para cada vari√°vel (ou coluna de X), √© calculado o res√≠duo (a diferen√ßa entre os valores reais e as previs√µes, excluindo a contribui√ß√£o da vari√°vel j).
O gradiente √© a derivada do erro em rela√ß√£o ao coeficiente theta[j], e √© calculado como o produto escalar entre a vari√°vel X_feature e o res√≠duo, normalizado pelo n√∫mero de amostras m.
O coeficiente theta[j] √© atualizado aplicando o soft thresholding (que faz a regulariza√ß√£o L1) no gradiente, com a penaliza√ß√£o L1 sendo controlada pelo par√¢metro self.l1_penalty.
A regulariza√ß√£o L1 ajuda a "zerar" coeficientes que s√£o pequenos, for√ßando o modelo a ser mais simples e com menos vari√°veis ativas.
            """


#4 CALCULO DO CUSTO 
#O custo combina o erro quadr√°tico m√©dio e a penaliza√ß√£o ùêø1

            # Calcula o custo atual
            #O custo √© uma combina√ß√£o do erro quadr√°tico m√©dio e da penaliza√ß√£o L1
            cost = self.cost(dataset)
            self.cost_history[iteration] = cost # Armazena o custo no hist√≥rico

#5 VERIFICA√á√ÉO EARLY STOPPING
#Objetivo: Parar o treinamento se o custo n√£o melhorar por v√°rias itera√ß√µes consecutivas.
#Se o custo da itera√ß√£o atual for maior ou igual ao da itera√ß√£o anterior, incrementa-se um contador. Quando esse contador atinge o valor de patience, o treinamento √© interrompido

            # Verifica se houve melhora no custo: early stopping
            # Early stopping baseado na aus√™ncia de melhoria no custo
            if iteration > 0 and cost >= self.cost_history[iteration - 1]: # Verifica se o custo n√£o melhorou
                early_stopping_counter += 1 # Incrementa o contador
                if early_stopping_counter >= self.patience: # Interrompe se atingir o limite de patience
                    break
            else:
                early_stopping_counter = 0 # Reseta o contador se o custo melhorar
        """
Se o custo da itera√ß√£o atual n√£o for menor do que o custo da itera√ß√£o anterior, o contador early_stopping_counter √© incrementado. Se o n√∫mero de itera√ß√µes sem melhora (early_stopping_counter) atingir o valor de patience, o treinamento √© interrompido.
Caso o custo melhore, o contador √© resetado para 0.
        """
        return self  # Retorna o modelo ajustado

    def cost(self, dataset: Dataset) -> float:
        """
        Calcula o custo do modelo.

        Par√¢metros:
        ----------
        dataset : Dataset
            Dados de entrada para c√°lculo do custo.

        Retorna:
        -------
        cost : float
            Valor do custo (erro quadr√°tico m√©dio + regulariza√ß√£o L1).
        """
#1 GERA√á√ÉO DAS PREVIS√ïES
#Aqui, o modelo utiliza os coeficientes atuais (ùúÉ) e o intercepto (ùúÉ0) para gerar as previs√µes (ùë¶pred) a partir dos dados ùëã fornecidos no conjunto dataset.
        y_pred = self.predict(dataset)  # Gera previs√µes para os dados
#A fun√ß√£o predict calcula essa f√≥rmula internamente, utilizando os coeficientes ùúÉ e ùúÉ0


#2 CALCULO DO ERRO QUADRATICO MEDIO(MSE)
        mse_error = np.mean((dataset.y - y_pred) ** 2) # Calcula o erro quadr√°tico m√©dio
#Objetivo: Minimizar o MSE para que as previs√µes ùë¶pred fiquem o mais pr√≥ximo poss√≠vel dos valores reais ùë¶

#3. PENALIZA√á√ÉO L1
        l1_term = self.l1_penalty * np.sum(np.abs(self.theta)) # Calcula a penaliza√ß√£o L1
#Objetivo: Adicionar um custo ao uso de coeficientes maiores


#4. CALCULO DO CUSTO TOTAL
        return mse_error / 2 + l1_term # Retorna o custo total


    def _predict(self, dataset: Dataset) -> np.ndarray:
        """
        Faz previs√µes para novos dados.

        Par√¢metros:
        ----------
        dataset : Dataset
            Dados de entrada.

        Retorna:
        -------
        predictions : np.ndarray
            Previs√µes geradas pelo modelo.
        """
#1 NORMALIZA√á√ÅO DOS DADOS
#Caso o par√¢metro scale seja True, os dados s√£o normalizados com base na m√©dia (self.mean) e no desvio padr√£o (self.std) calculados durante o treinamento.
#Caso scale seja False, os dados s√£o usados como est√£o.
        X = (dataset.X - self.mean) / (self.std + 1e-8) if self.scale else dataset.X # Normaliza se necess√°rio

#2 CALCULO DAS PREVIS√ïES
#X‚ãÖŒ∏: Produto escalar entre os dados normalizados (ùëã) e os coeficientes (ùúÉ).
#Œ∏ : Intercepto adicionado ao resultado.
        return np.dot(X, self.theta) + self.theta_zero  # Calcula as previs√µes


    def _score(self, dataset: Dataset, predictions: np.ndarray) -> float:
        """
        Avalia o modelo com base no erro m√©dio quadr√°tico (MSE).

        Par√¢metros:
        ----------
        dataset : Dataset
            Conjunto de dados para avalia√ß√£o.
        predictions : np.ndarray
            Previs√µes do modelo.

        Retorna:
        -------
        mse : float
            Erro m√©dio quadr√°tico.
        """

        return mse(dataset.y, predictions) # Retorna o MSE entre as previs√µes e os valores reais
#Entrada: Valores reais (ùë¶) e previs√µes (ùë¶pred)
#Sa√≠da: Erro m√©dio quadr√°tico (ùëÄùëÜùê∏)


    def _apply_soft_threshold(self, value: float, penalty: float) -> float:
        """
        Aplica a t√©cnica de soft thresholding para regulariza√ß√£o L1.

        Par√¢metros:
        ----------
        value : float
            Gradiente calculado para um coeficiente.
        penalty : float
            Penaliza√ß√£o L1.

        Retorna:
        -------
        thresholded_value : float
            Valor ajustado ap√≥s a regulariza√ß√£o.
        """
        if abs(value) > penalty: # Verifica se o valor est√° fora do intervalo de penaliza√ß√£o
            return np.sign(value) * (abs(value) - penalty)   # Aplica a redu√ß√£o pelo soft thresholding
        return 0.0 # Zera o valor se estiver dentro do intervalo de penaliza√ß√£o
#Entrada: Valor do coeficiente (value) e a penalidade (penalty).
#Sa√≠da: Valor ajustado ap√≥s aplica√ß√£o do Soft Thresholding.